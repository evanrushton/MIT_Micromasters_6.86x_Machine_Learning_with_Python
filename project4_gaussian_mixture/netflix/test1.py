import numpy as np
from scipy import stats as st

def norm_pdf(X, mu, var):
    shape = X.shape
    _, d = shape
    return 1/((2*np.pi*var)**(d/2)) * np.exp(-np.divide(np.linalg.norm(X - mu, axis=1)**2,2*var))

def norm(x, mu, var):
    return 1/np.sqrt(2*np.pi*var) * np.exp(-np.divide(np.linalg.norm(x - mu)**2,2*var))


X = np.array([[0.85794562, 0.84725174], [0.6235637, 0.38438171], [0.29753461, 0.05671298], [0.27265629, 0.47766512], [0.81216873, 0.47997717], [0.3927848, 0.83607876], [0.33739616, 0.64817187], [0.36824154, 0.95715516], [0.14035078, 0.87008726], [0.47360805, 0.80091075], [0.52047748, 0.67887953], [0.72063265, 0.58201979], [0.53737323, 0.75861562], [0.10590761, 0.47360042], [0.18633234, 0.73691818]])
K = 6
post = np.array([[0.15765074, 0.20544344, 0.17314824, 0.15652173, 0.12169798, 0.18553787], [0.1094766, 0.22310587, 0.24109142, 0.0959303, 0.19807563, 0.13232018], [0.22679645, 0.36955206, 0.02836173, 0.03478709, 0.00807236, 0.33243031], [0.16670188, 0.18637975, 0.20964608, 0.17120102, 0.09886116, 0.16721011], [0.04250305, 0.22996176, 0.05151538, 0.33947585, 0.18753121, 0.14901275], [0.09799086, 0.28677458, 0.16895715, 0.21054678, 0.0069597, 0.22877093], [0.16764519, 0.16897033, 0.25848053, 0.18674186, 0.09846462, 0.11969746], [0.28655211, 0.02473762, 0.27387452, 0.27546459, 0.08641467, 0.05295649], [0.11353057, 0.13090863, 0.20522811, 0.15786368, 0.35574052, 0.03672849], [0.10510461, 0.08116927, 0.3286373, 0.12745369, 0.23464272, 0.12299241], [0.09757735, 0.06774952, 0.40286261, 0.08481828, 0.1206645, 0.22632773], [0.24899344, 0.02944918, 0.25413459, 0.02914503, 0.29614373, 0.14213403], [0.35350682, 0.21890411, 0.26755234, 0.01418274, 0.10235276, 0.04350123], [0.15555757, 0.06236572, 0.16703133, 0.21760554, 0.03369562, 0.36374421], [0.1917808, 0.08982788, 0.17710673, 0.03179658, 0.19494387, 0.31454414]])
mu = np.array([[0.6235637, 0.38438171], [0.3927848, 0.83607876], [0.81216873, 0.47997717], [0.14035078, 0.87008726], [0.36824154, 0.95715516], [0.10590761, 0.47360042]])
var = np.array([0.10038354, 0.07227467, 0.13240693, 0.12411825, 0.10497521, 0.12220856])
p = np.array([0.1680912, 0.15835331, 0.21384187, 0.14223565, 0.14295074, 0.17452722])

def E_step1(X, mu, var, p):
    n,_ = X.shape
    K,_ = mu.shape
    marginal_dist = np.float64(np.zeros((n,K)))
    post = np.float64(np.zeros((n,K)))
    for j in range(K):
        marginal_dist[:,j] = p[j]*norm_pdf(X,mu[j],var[j])
    L = np.sum(marginal_dist, axis=1)
    for j in range(K):
       post[:,j]=marginal_dist[:,j] / L # divide by rowsum to get multinomial 
    LL = np.sum(np.log(L))

    return post, LL

def M_step(X, post):
    """M-step: Updates the gaussian mixture by maximizing the log-likelihood
    of the weighted dataset

    Args:
        X: (n, d) array holding the data
        post: (n, K) array holding the soft counts
        for all components for all examples

    Returns:
        GaussianMixture: the new gaussian mixture
    """
    n,d = X.shape
    _,K = post.shape

    n_hat = np.sum(post, axis=0) # ( , K)
    p = n_hat / n
    mu =  np.multiply(np.atleast_2d(1/n_hat).T, post.T @ X)
    var = np.sum(np.multiply(np.multiply(1/(n_hat*d), post), (np.linalg.norm(X-mu[:, np.newaxis], axis=2)**2).T), axis=0)
    
    return mu, var, p

incorrect = np.array([[0.00641731, 0.0071769, 0.01410221, 0.00210437, 0.00600918, 0.00116822], [0.02452702, 0.00541234, 0.0199811, 0.00253339, 0.00306467, 0.00676456], [0.00846168, 0.00045102, 0.00442368, 0.00105747, 0.00040958, 0.00884029], [0.0127191, 0.01194237, 0.00788088, 0.00841187, 0.00638821, 0.01866761], [0.01963036, 0.00395345, 0.02365616, 0.0014759, 0.00263755, 0.00271735], [0.006809, 0.03209248, 0.00754277, 0.0129249, 0.01854777, 0.00872627], [0.01153382, 0.02460939, 0.00907568, 0.01177197, 0.01260106, 0.01483055], [0.00345905, 0.02887682, 0.00475687, 0.01320725, 0.01994628, 0.00606412], [0.00236723, 0.02048682, 0.00242194, 0.01678553, 0.01502283, 0.01094184], [0.00924057, 0.03041312, 0.01040028, 0.01052593, 0.01684218, 0.00776118], [0.01510244, 0.024164, 0.01477492, 0.00809418, 0.01235202, 0.0087149], [0.01926491, 0.0097621, 0.02203566, 0.00309489, 0.00564792, 0.00424801], [0.01176576, 0.02664182, 0.01326707, 0.00846105, 0.01442618, 0.00700485], [0.00620532, 0.0073178, 0.00359615, 0.00886813, 0.00471873, 0.02091817], [0.0050964, 0.02232556, 0.00420081, 0.01549566, 0.01352313, 0.01534017]])

correct = np.array([[0.17354324, 0.19408461, 0.38136556, 0.0569083, 0.16250611, 0.03159219], [0.39379907, 0.08689908, 0.32081103, 0.04067548, 0.04920547, 0.10860986], [0.35788286, 0.01907566, 0.18709725, 0.04472511, 0.01732312, 0.37389601], [0.19268431, 0.18091751, 0.11938917, 0.12743323, 0.09677628, 0.28279951], [0.36304946, 0.07311615, 0.43750366, 0.02729566, 0.04877955, 0.05025552], [0.07858663, 0.37039817, 0.08705556, 0.14917384, 0.21407078, 0.10071502], [0.13662023, 0.29150288, 0.10750309, 0.13944117, 0.14926196, 0.17567066], [0.04532867, 0.37841271, 0.06233585, 0.17307275, 0.2613835, 0.07946652], [0.03479877, 0.30116079, 0.03560306, 0.24675099, 0.22083886, 0.16084754], [0.1084787, 0.35703165, 0.12209296, 0.12356811, 0.19771701, 0.09111156], [0.18151437, 0.29042408, 0.1775779, 0.09728296, 0.14845737, 0.10474333], [0.30076285, 0.15240546, 0.34401968, 0.04831719, 0.08817504, 0.06631978], [0.14424702, 0.32662602, 0.16265301, 0.10373169, 0.17686354, 0.08587872], [0.12020157, 0.14175102, 0.06966009, 0.17178204, 0.09140514, 0.40520014], [0.06707408, 0.29382796, 0.05528713, 0.20393925, 0.17797873, 0.20189285]])

### TEST M-step
mu, var, p = M_step(X, post)
print(mu, var, p)
print('Mu: [[0.43216722 0.64675402] [0.46139681 0.57129172] [0.44658753 0.68978041] [0.44913747 0.66937822] [0.47080526 0.68008664] [0.40532311 0.57364425]]' + '\n' + 'Var: [0.05218451 0.06230449 0.03538519 0.05174859 0.04524244 0.05831186]' + '\n' + 'P: [0.1680912  0.15835331 0.21384187 0.14223565 0.14295074 0.17452722]')



### TEST E-Step
#post1, LL1 = E_step1(X, mu, var, p)
#post2, LL2 = E_step2(X, mu, var, p)
#print(post1)
#print(np.sum(post1, axis=1))
#print(correct)
#print("Log-likelihood: ", LL1)
#if (post1 == correct).all():
#    print("CORRECT!")
#else: print("INCORRECT")

